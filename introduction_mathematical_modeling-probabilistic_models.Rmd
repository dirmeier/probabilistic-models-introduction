---
title: "Introduction to Probabilistic Models"
output:
  html_document
---

```{r setup, include=FALSE}
  knitr::opts_chunk$set(echo = TRUE)
```

<br><br>

This document contains some short demos for a probabilistic models exercise. 
The demos are practical applications (maximum likelihood/optimization, Bayesian networks) of the content taught
in the exercise to familiarize people with the the theoretical concepts. The demos are not necessarily exact reproductions of the exercises.

<br><br>

### Maximum Likelihood for binomial distributions

The binomial distribution models the number of successes $k$ in $n$ Bernoulli trials. Let's have a look
how the distribution looks for different success probabilities $p$.

```{r}
  plot(0:10,
       dbinom(0:10, 10, prob=.1),
       col=rgb(0, 0 , 1, .5),
       xlim=c(0, 10), type="h", lwd=3,
       main="Binomial distribution with different success probabilities",
       xlab="Count success",
       ylab="Frequency")
  points(0:10, dbinom(0:10, 10, prob=.5),
         col=rgb(0, 1, 0, .5), type="h", lwd=3)
  points(0:10, dbinom(0:10, 10, prob=.8),
         col=rgb(1, 0, 0, .5), type="h", lwd=3)
  legend("topright", col=c(rgb(0, 0, 1, .5), rgb(0, 1, 0, .5), rgb(1, 0, 0, .5)),
         lwd=3, paste0("p=", c(.1, .5, .8)))
```

To find the *maximum likelihood estimate* for $p$ we can either set the derivative of the likelihood function to $0$, or do it computationally.

First, we define the likelihood:
```{r}
  binom.likelihood <- function(p, n, k)
  {
    dbinom(x=k, size=n, prob=p)
  }
```

Then we set some data:
```{r}
  tails <- 0
  heads <- 1
  tosses <- c(tails, heads, tails, tails, tails,
              heads, heads, tails, heads, tails,
              tails, heads, tails, tails, heads)

  n <- length(tosses)
  k <- sum(tosses == heads)
```

Then we `optimize` the likelihood function, i.e., find its maximum. (For functions with multiple variables use `optim`). 
Usually we use the *negative log-likelihood* for optimization, and then find its minimum. 
The two approaches are however equivalent.

```{r}
  p.mle <- optimize(binom.likelihood,
                    interval=c(0, 1),
                    n=n, k=k,
                    maximum=TRUE)$maximum

  cat(paste0("The MLE of p=", p.mle))
  lik <-  binom.likelihood(p.mle, n, k)
  cat(paste0("The Likelihood is ", lik))
```

Plot the results:
```{r}
  plot(seq(0, 1, length.out=100), 
       sapply(seq(0, 1, length.out=100), binom.likelihood,n=n, k=k),
       col="orange", ylim=c(0, 1), xlab="p", ylab="Likelihood", type="l")

  points(p.mle, binom.likelihood(p.mle, n, k) , col="red", lwd=3 )
  abline(v = p.mle, col=1, lty=4)
  abline(h = binom.likelihood(p.mle, n, k), col=1, lty=4)
  legend("topright", col=c("orange", "red"), lty=c(1, 0), lwd=c(1, 3),
         pch=c(-1, 1), c("Likelihood function", "MLE"), box.lty=0)
```


### Poisson distribution

The Poisson distribution models the number of events $k$ that happen in a fixed time-interval. Let's visualize it again first.

```{r}
  plot(0:20,
       dpois(0:20, lambda=1),
       col=rgb(0, 0, 1, .5),
       xlim=c(0, 20), type="h", lwd=3,
       main="Poisson distribution with different mean/lambdas",
       xlab="Lambda",
       ylab="Probability")
  points(0:20, dpois(0:20, lambda=5),
         col=rgb(0, 1 , 0, .5), type="h", lwd=3)
  points(0:20, dpois(0:20, lambda=8),
         col=rgb(1, 0 , 0, .5), type="h", lwd=3)
  legend("topright", col=c(rgb(0, 0, 1, .5), rgb(0, 1 , 0, .5), rgb(1, 0, 0, .5)) ,
         lwd=3, c(expression(paste(lambda,"=", 1)),
                  expression(paste(lambda,"=", 5)),
                  expression(paste(lambda,"=", 8))))
```

First, we define the Poisson likelihood function, as before:
```{r}
  poisson.likelihood <- function(x, colonies)
  {
    prod(sapply(colonies, function(c) dpois(x=c, lambda=x)))
  }
```

Then let's evaluate the likelihood for $3$ colonies and $\lambda = 5$:
```{r}
  colonies <- 3
  
  lik <-  poisson.likelihood(5, colonies)
  cat(paste0("The Likelihood is for lambda=5 is: ", lik))
```

**NOTE**: generally it is a bad idea to rely on the MLE with such a low sample size (here $1$).
For that reason we use some example data for the next steps.
```{r}
  colonies <- c(1,2,4,5,7,2,3,5,6,3,7,2)
  colonies
```

Furthermore, since we compute products of probabilities, it makes sense to work in a log-space. Thus, we define the log-likelihood: 
```{r}
  poisson.log.likelihood <- function(x, colonies)
  {
    sum(sapply(colonies, function(c) log(dpois(x=c, lambda=x))))
  }
```
As noted before, usually people do optimization on the negative log-likelihood, but it really doesn't matter.

Then let's optimize the likelihood with the same procedure as before.
```{r}
  p.mle <- optimize(poisson.log.likelihood,
                    interval=c(0, 1000),
                    colonies=colonies,
                    maximum=TRUE)$maximum

  cat(paste0("The MLE of p=", p.mle))
```

We can quickly check if this is true, because the MLE of a Poisson distribution for $\lambda$ is the mean of the colonies:
```{r}
  m.c <- mean(colonies)
  cat(paste0("The mean of the colonies is: ", m.c))
```
That worked just fine. If you see differences in some of the digits after the comma, this is only due to numerical reasons. Nothing to worry about.

Finally, let's plot the results again:
```{r}
  plot(seq(0, 20, length.out=1000), 
       sapply(seq(0, 20, length.out=1000), poisson.log.likelihood, colonies=colonies),
        col="orange", xlab="p", ylab="Likelihood", type="l")

  points(p.mle, poisson.log.likelihood(p.mle, colonies) , col="red", lwd=3 )
  abline(v = p.mle, col=1, lty=4)
  abline(h = poisson.log.likelihood(p.mle, colonies), col=1, lty=4)
  legend("bottomright", col=c("orange", "red"), lty=c(1, 0), lwd=c(1, 3),
         pch=c(-1, 1), c("Likelihood function", "MLE"), box.lty=0)
```

If you look at the plot, you should see that the y-axis is negative now. This is due to the fact, that we took the logarithm on the likelihood. So that is normal.

### Markov Chains

Finding the stationary distribution of an MC, is essentially an Eigenvalue problem, where we look for the left Eigenvectors of a transition matrix for the Eigenvalue $1$.

First, we setup the transition matrix:
```{r}
  T <- matrix(c(.4, .9, .6, .1), ncol=2)
```

Now, we need to find the *left* Eigenvectors of `T`. This is equivalent to finding the *right* Eigenvectors of the transpose of `T`: `t(T)`. 
The stationary distribution is *unique* for ergodic Markov chains and independent of the starting distribution. 
```{r}
  ev <- eigen(t(T))
  ev
  eigen.values <- ev$vectors[,which(ev$values == 1)]
```
  
We find the stationary distribution by normalizing the respective eigenvector to sum to `1`.
```{r}  
  stationary.distribution <- eigen.values / sum(eigen.values)
  stationary.distribution
```

### Inference in Bayesian Networks

To work with Bayesian networks we first load some libraries:

```{r, echo=TRUE, include=TRUE, warning=FALSE, message=FALSE}
  library(bnlearn)
  library(tibble)
```

Then, we create the data just as in the exercise:
```{r}
  df <- data.frame(
    A = as.character(c(0, 0, 0, 1, 1, 0, 1, 1, 1)),
    B = as.character(c(0, 0, 0, 1, 0, 0, 1, 0, 0)),
    C = as.character(c(0, 0, 0, 1, 1, 0, 1, 0, 0)),
    D = as.character(c(0, 0, 0, 1, 1, 1, 1, 0, 1)),
    E = as.character(c(0, 1, 1, 0, 0, 1, 0, 1, 1))
  )

  as.tibble(df)
```

We define the Bayesian network structure also as in the exercise (we could also learn this from the data):
```{r}
  dag <- empty.graph(toupper(letters[1:5]))
  arc.set <- matrix(
    c("A", "B", "A", "D", "B", "C", "E", "B"),
    ncol = 2, byrow = TRUE,
    dimnames = list(NULL, c("from", "to")))
  arcs(dag) <- arc.set
  plot(dag)
```

Then, we compute the local probability tables (the parameters):
```{r}
  fit <- bn.fit(dag, df)
  fit
```

With this, we are finally able submit some *queries* find the maximum conditional probability as in the exercise:
```{r}
  cpquery(fit, (A == "1"), (C == "1"))
  cpquery(fit, (A == "0"), (C == "1"))

  cpquery(fit, (E == "1"), (C == "1"))
  cpquery(fit, (E == "0"), (C == "1"))
```


<br><br><br>&copy; *S. D.* (<a href="https://github.com/dirmeier/probabilistic-models-introduction">GitHub</a>)
