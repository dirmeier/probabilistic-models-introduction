---
title: "Introduction to Probabilistic Models"
---

```{r setup, include=FALSE}
  knitr::opts_chunk$set(echo = TRUE, fig.align='center', fig.width=10, warning=FALSE)
  library(tidyverse)
```

This document contains some short demos for a probabilistic models exercise. 
The demos are practical applications (maximum likelihood/optimization, Bayesian networks) of the content taught
in the exercise to familiarize people with the the theoretical concepts. The demos are not necessarily exact reproductions of the exercises.

# Tossing coins

The binomial distribution models the number of successes $k$ in $n$ Bernoulli trials, each having a success probability of $p$:
$$
X \sim \text{Binom}(n, p)
$$
The easiest to understand what it is doing is to think of $n$ coin tosses where every *head* is a success. The sum of the heads is the value of the Binomial random variable. A normal coin has a success probability of $p = 0.5$. If we use an unfair coin, it might have a probability of landing on its head like $p=0.3$ or $p=0.8$. Let's have a look how the distribution of *sum of heads* looks for different success probabilities $p$ and $10$ coin tosses:

```{r, fig.align='center', echo=FALSE}
tab <- purrr::map_df(c(0.3, 0.5, 0.8), function(p) {
  data.frame(x=0:10, probs=dbinom(0:10, 10, prob=p), p=p)
})
ggplot(tab) +
  geom_col(aes(x=x, y=probs, fill=factor(p)), alpha=.75, width = .75, position = "dodge") +
  scale_x_continuous("Number of heads", breaks=seq(0, 10)) +
  scale_y_continuous("Probability of seeing 'x' heads") +
  scale_fill_discrete("Probability of head\nper coin toss") +
  ggthemes::theme_tufte()  +
  theme(axis.text=element_text(colour="grey20"),
        axis.title=element_text(colour="grey20"),
        axis.ticks=element_line(colour="grey20"))
```

Usually, however, we only observe a finite sample of which we don't know its parameter, thus we need to *guess* them. What's do you think is the parameter in 
the example below?

```{r, fig.align='center', echo=FALSE}
set.seed(3)
tab <-  data.frame(samples=rbinom(100, 10, 0.7))

ggplot(tab) +
  geom_histogram(aes(samples, y = ..density..), alpha=.75, bins=11, color="darkgrey") +
  scale_x_continuous("Number of heads", breaks=seq(0, 10), limits=c(0, 10)) +
  scale_y_continuous("Probability of seeing 'x' heads") +
  scale_fill_discrete("Probability of head\nper coin toss") +
  ggthemes::theme_tufte() +
  theme(axis.text=element_text(colour="grey20"),
        axis.title=element_text(colour="grey20"),
        axis.ticks=element_line(colour="grey20"))
```

For this simulation I set the success probability to $p = 0.7$, even though the data would imply $p=0.8$ (just look at the largest bar and divide by size $n=10$). Thus, **beware**, since we mostly look at samples of small size, we sometimes make wrong guesses about the parameter of interest.

## Maximum likelihood

To find the *maximum likelihood estimate* - some guess - for $p$ we can either set the derivative of the likelihood function to $0$ (as we'll do it in the exercise), or do it computationally. Let's do the analytical derivation first. The likelihood we are interested in optimizing is given by:

$$
\mathcal{L}(p)= {n\choose k} p^k(1-p)^{n-k}
$$
Taking the log yields:
$$
\ell(p) = \log {n \choose k}+k \log(p)+(n-k)\log(1-p)
$$
To obtain the MLE we need to derive $ell(p)$ and solve for zero:
$$
\ell'(p) = \frac{k}{p} -\frac{n-k}{1-p} = 0 \Rightarrow 0=\frac{k(1-p)-(n-k)p}{p(1-p)}= \frac{k-np}{p-p^2} \Rightarrow p=\frac{k}{n}.
$$

Computationally, we do the same. First, we define the likelihood:
```{r}
binom.likelihood <- function(p, n, k)
{
  dbinom(x=k, size=n, prob=p)
}
```

Then we set some data:
```{r}
tails <- 0
heads <- 1
tosses <- c(tails, heads, tails, tails, tails,
            heads, heads, tails, heads, tails,
            tails, heads, tails, tails, heads)

n <- length(tosses)
k <- sum(tosses == heads)
```

Then we `optimize` the likelihood function, i.e., find its maximum. (For functions with multiple variables use `optim`). 
Usually we use the *negative log-likelihood* for optimization, and then find its minimum. 
The two approaches are however equivalent.

```{r}
p.mle <- optimize(binom.likelihood,
                  interval=c(0, 1),
                  n=n, k=k,
                  maximum=TRUE)$maximum

cat(paste0("The MLE using 'optimize' of p=", p.mle))
cat(paste0("The analytical solution for the MLE of p=", k/n))
```

Plot the likelihood function and its maximum:
```{r asdaa, echo=FALSE}
par(family = "serif")
curve(binom.likelihood(x, n=n, k=k), from=0, to=1, 
      col="orange", ylim=c(0, 1), type="l",  axes=F,
      xlab="p", ylab=expression("l(p)"), col.lab="grey20")
axis(1, at=seq(0.1, .9, by=.1), label=seq(0.1, .9, by=.1), tick=T, 
     family="serif", lwd = 0, lwd.ticks = 1, tck=-0.01, col="grey20")
axis(1, at=seq(0.05, 0.95, by=.1), label=NA,  tick=T, 
     family="serif", lwd.ticks = NA)
axis(2, at=seq(0.1, .9, by=.1), label=seq(0.1, .9, by=.1), tick=T, 
     family="serif", lwd = 0, lwd.ticks = 1, tck=-0.01, col="grey20")
axis(2, at=seq(0.05, 0.95, by=.1), label=NA,tick=T, 
     family="serif", lwd.ticks = NA)
points(p.mle, binom.likelihood(p.mle, n, k) , col="red", lwd=3)
abline(v = p.mle, col=1, lty=4)
abline(h = binom.likelihood(p.mle, n, k), col=1, lty=4)
legend("topright", col=c("orange", "red"), lty=c(1, 0), lwd=c(1, 3), 
       text.col="grey20", pch=c(-1, 1), c("Likelihood function", "MLE"), box.lty=0)
```

## Bayesian inference

We can alternatively compute an estimate of $p$ using a Bayesian approach. Here, we would think of our parameter as a random variable that has a ddistribution. Since we are looking at a parameter that has domain $p = [0, 1]$ a reasonable prior *could* be a Beta distribution (because of its domain and the fact that it's conjugate). The plot below visualizes the Beta distribution for some hyperparameter settings.

```{r sda, echo=FALSE}
x <- seq(0, 1, length.out=1000)
tab <- dplyr::bind_rows(list(
  data.frame(Parameters="a=0.5, b=0.5", x=x, prob=dbeta(x, .5, 0.5)),
  data.frame(Parameters="a=1, b=2", x=x, prob=dbeta(x, 1, 2)),
  data.frame(Parameters="a=2, b=1", x=x, prob=dbeta(x, 2, 1)),
  data.frame(Parameters="a=1, b=1", x=x, prob=dbeta(x, 1, 1))))

ggplot(tab) +
  geom_line(aes(x=x, y=prob, color=Parameters)) +
  scale_x_continuous("p") +
  scale_y_continuous("Density of p") +
  ggthemes::theme_tufte(base_size=13) +
  ggthemes::geom_rangeframe(aes(x=x, y=prob)) +
  theme(axis.text=element_text(colour="grey20"),
        axis.title=element_text(colour="grey20"),
        axis.ticks=element_line(colour="grey20")) +
  scale_color_manual(values=c("orange", 1, 1, 1)) +
  ggtitle("Different prior distributions")
```

The difference here to the frequentist approach, among other things, is that our estimate of the parameter is again a distribution, the *posterior*. In this example we use a prior with hyperparameters $a=0.5$ and $b=0.5$. Since the Beta distribution is conjugate to the Binomial, the posterior will also be a Beta distribution:

```{r}
posterior <- function(p, n, k) 
{
 dbeta(p, 0.5 + k, 0.5 + n - k)
}

posterior_values <- sapply(x, function(.) posterior(., n, k))
```

```{r, echo=FALSE}
tab <- data.frame(x=x, probs=posterior_values)
ggplot() +
    geom_line(data = tab, aes(x=x, y=probs)) +
    scale_x_continuous("p") +
    scale_y_continuous("Density of p") +
    ggthemes::theme_tufte(base_size=13) +
    ggthemes::geom_rangeframe(data = tab, aes(x=x, y=probs)) +
    theme(axis.text=element_text(colour="grey20"),
          axis.title=element_text(colour="grey20"),
          axis.ticks=element_line(colour="grey20")) +
  geom_vline(data=NULL, xintercept=x[which.max(posterior_values)], color="orange") +
  geom_text(data=NULL, aes(x=x[which.max(posterior_values)], label=paste0("Mode at: ", sprintf("%.2f",  x[which.max(posterior_values)])), y=1), hjust=-.15) +
  ggtitle("Posterior distribution")
```

Thus Bayesian inference of a parameter can be thought of as *analysis of beliefs* ([Larry Wasserman](https://normaldeviate.wordpress.com/2012/11/17/what-is-bayesianfrequentist-inference/)), where we update our knowledge of some parameter (*prior*) in the context of data (giving us the *posterior*).

# Bacteria and Petri dishes

The Poisson distribution models the number of events $k$ that happen in a fixed time-interval. It is usually parametrized by a mean value $\lambda$:
$$
X \sim \text{Pois}(\lambda)
$$
One (very abstract) example to think of the Poisson distribution is the number of bacterial colonies in different Petri dishes. 
On average we would expect to observe $\lambda$ many colonies per dish, however due to stochasticity, such as food availability or space to grow the numbers per dish might vary.

Let's visualize it again first.
```{r, fig.align='center', echo=FALSE}
tab <- purrr::map_df(c(1, 5, 10), function(p) {
  data.frame(x=0:20, probs=dpois(0:20, lambda=p), lambda=p)
})
ggplot(tab) +
  geom_col(aes(x=x, y=probs, fill=factor(lambda)), alpha=.75, width = .75, position = "dodge") +
  scale_x_continuous("Number of colonies", breaks=seq(0, 20)) +
  scale_y_continuous("Probability of seeing 'x' colonies") +
  scale_fill_discrete("Mean number of colonies") +
  ggthemes::theme_tufte(base_size=13) +
  theme(axis.text=element_text(colour="grey20"),
        axis.title=element_text(colour="grey20"),
        axis.ticks=element_line(colour="grey20"))
```

## Maximum likelihood

As before we usually do not know the true parameter that governs the number of colonies, so we have to estimate it from a finite sample.
And as before we choose to use the maximum likelihood estimator to find the mass function's parameter which in this case is $\lambda$.

Let's again do the analytical derivation first. The likelihood we are interested in optimizing is given by:

$$ \mathcal{L} (\lambda) = \prod_{i=1}^n f(k_i; \lambda)= \prod_{i=1}^n \frac{\lambda^{k_i} e^{-\lambda}}{k_i!},$$	
where $k_i$ are the bacterial colonies per dish $i$.

Taking the log yields:
\begin{align}
\ell(\lambda) &= \log \prod_{i=1}^n \frac{\lambda^{k_i} e^{-\lambda}}{k_i!}  \\ 
& = \sum_{i=1}^n \log \left(\frac{\lambda^{k_i} e^{-\lambda}}{k_i!} \right)= \sum_{i=1}^n \left( -\lambda +k_i \log(\lambda) - \log(k_i!) \right) \\
&= -n \lambda + \log(\lambda) \sum_{i=1}^n k_i - \sum_{i=1}^n \log(k_i!)
\end{align}

To obtain the MLE we need to derive $\ell(p)$ and solve for zero:
$$
\ell'(\lambda)=-n+\frac{1}{\lambda}\sum_{i=1}^n k_i =0 \Rightarrow \lambda=\frac{1}{n}\sum_{i=1}^n k_i.
$$
If we do this computationally we define the Poisson likelihood function, as before the Binomial. Here we have to take the product over all bacterial colonies:
```{r}
poisson.likelihood <- function(x, colonies)
{
  sapply(x, function(l) prod(dpois(x=colonies, lambda=l)))
}
```

Then let's evaluate the likelihood for $3$ colonies and $\lambda = 5$:
```{r}
colonies <- 3

lik <-  poisson.likelihood(5, colonies)
cat(paste0("The Likelihood for lambda=5 is: ", lik))
```

**NOTE**: generally it is a bad idea to rely on the MLE with such a low sample size (here $1$). 
For that reason we use some example data for the next steps.

```{r}
colonies <- c(1 ,2, 4, 5, 7, 2, 3, 5, 6, 3, 7, 2)
```

Furthermore, since we compute products of probabilities, it makes sense to work in a log-space. Thus, we define the log-likelihood: 
```{r}
poisson.log.likelihood <- function(x, colonies)
{
  sapply(x, function(l) sum(log(dpois(x=colonies, lambda=l))))
}
```

Then let's optimize the likelihood with the same procedure as before.
```{r}
p.mle <- optimize(poisson.log.likelihood,
                  interval=c(0, 1000),
                  colonies=colonies,
                  maximum=TRUE)$maximum

cat(paste0("The MLE using 'optimize' of lambda=", p.mle))
```

We can quickly check if this is true, because the MLE of a Poisson distribution for $\lambda$ is the mean of the colonies:
```{r}
m.c <- mean(colonies)
cat(paste0("The mean of the colonies is: ", m.c))
```
That worked just fine. If you see differences in some of the digits after the comma, this is only due to numerical reasons. Nothing to worry about.

Let's plot the likelihood function and its maximum again:
```{r, echo=FALSE}
par(family = "serif")
curve(exp(poisson.log.likelihood(x, colonies=colonies)), from=0, to=10,
      col="orange", xlab=expression(lambda), type="l", axes=F,
      ylab=expression("l(p)"), col.lab="grey20")
axis(1, at=seq(0, 10, by=1), label=seq(0, 10, by=1), tick=T, 
     family="serif", lwd = 0, lwd.ticks = 1, tck=-0.01, col="grey20")
axis(1, at=seq(.5, 9, by=1), label=NA,  tick=T, 
     family="serif", lwd.ticks = NA)
axis(2, at=c(0, exp(poisson.log.likelihood(p.mle, colonies))),
     label=c(0, exp(poisson.log.likelihood(p.mle, colonies))), tick=T, 
     family="serif", lwd = 0, lwd.ticks = 1, tck=-0.01, col="grey20")
points(p.mle, exp(poisson.log.likelihood(p.mle, colonies)), col="red", lwd=3 )
abline(v = p.mle, col=1, lty=4)
abline(h = exp(poisson.log.likelihood(p.mle, colonies)), col=1, lty=4)
legend("topright", col=c("orange", "red"), lty=c(1, 0), lwd=c(1, 3),
       pch=c(-1, 1), c("Likelihood function", "MLE"), box.lty=0)
```

## Bayesian inference

As before, we could infer the parameters in a Bayesian fashion. In order to avoid sampling or variational approximations, we go with a conjugate prior again, which for a Poisson model is the Gamma distribution:

```{r, echo=FALSE}
x <- seq(0, 20, length.out=1000)
tab <- dplyr::bind_rows(list(
  data.frame(Parameters="shape=1", x=x, prob=dgamma(x, 1)),
  data.frame(Parameters="shape=2", x=x, prob=dgamma(x, 2)),
  data.frame(Parameters="shape=10", x=x, prob=dgamma(x, 10)),
  data.frame(Parameters="shape=5", x=x, prob=dgamma(x, 5))))

ggplot(tab) +
  geom_line(aes(x=x, y=prob, color=Parameters)) +
  scale_x_continuous(expression(lambda)) +
  scale_y_continuous("Density") +
  ggthemes::theme_tufte(base_size=13) +
  ggthemes::geom_rangeframe(aes(x=x, y=prob)) +
  theme(axis.text=element_text(colour="grey20"),
        axis.title=element_text(colour="grey20"),
        axis.ticks=element_line(colour="grey20")) +
  scale_color_manual(values=c("orange", 1, 1, 1)) + 
ggtitle("Different prior distributions")
```

The assume a Gamma prior with shape $\alpha = 1$ and scale $\beta = 1$. The posterior is then computed as:

```{r}
posterior <- function(lambda, d) 
{
  dgamma(lambda, 1 + sum(d), 1 + length(d))
}

posterior_values <- sapply(x, function(.) posterior(., colonies))
```

```{r, echo=FALSE}
tab <- data.frame(x=x, probs=posterior_values)
ggplot() +
    geom_line(data = tab, aes(x=x, y=probs)) +
    scale_x_continuous(expression(lambda)) +
    scale_y_continuous("Density") +
    ggthemes::theme_tufte(base_size=13) +
    ggthemes::geom_rangeframe(data = tab, aes(x=x, y=probs)) +
    theme(axis.text=element_text(colour="grey20"),
          axis.title=element_text(colour="grey20"),
          axis.ticks=element_line(colour="grey20")) +
  geom_vline(data=NULL, xintercept=x[which.max(posterior_values)], color="orange") +
  geom_text(data=NULL, aes(x=x[which.max(posterior_values)], label=paste0("Mode at: ", sprintf("%.2f",  x[which.max(posterior_values)])), y=1), hjust=-.15) +
  ggtitle("Posterior distribution")
```

# Markov Chains

Finding the stationary distribution of an MC, is essentially an Eigenvalue problem, where we look for the left Eigenvectors of a transition matrix for the Eigenvalue $1$.

First, we setup the transition matrix:
```{r}
T <- matrix(c(.4, .9, .6, .1), ncol=2)
```

Now, we need to find the *left* Eigenvectors of `T`. This is equivalent to finding the *right* Eigenvectors of the transpose of `T`: `t(T)`. 
The stationary distribution is *unique* for ergodic Markov chains and independent of the starting distribution. 
```{r}
ev <- eigen(t(T))
ev
eigen.values <- ev$vectors[,which(ev$values == 1)]
```
  
We find the stationary distribution by normalizing the respective eigenvector to sum to `1`.
```{r}  
stationary.distribution <- eigen.values / sum(eigen.values)
stationary.distribution
```

We can also do this by hand, but for larger systems this is inefficient:
\begin{align*}
\text{(I) } &\; 0.4 \pi_1 + 0.9 \pi_2 = \pi_1\\
&\;   \Rightarrow   \pi_1 = \frac{0.9}{0.6} \pi_2 \\
\text{(II) } & \;  0.6 \pi_1 + 0.1 \pi_2 = \pi_2\\
\text{(III) } & \;  \pi_1 + \pi_2 = 1\\
\text{Plugging (I) into (III): } & \; \frac{0.9}{0.6} \pi_2 + \pi_2 = 1\\ 
				&\;   \Rightarrow   \pi_2 = 0.4 \\
				&\;   \Rightarrow   \pi_1 = 0.6 \\
\end{align*}

# Bayesian networks

To work with Bayesian networks we first load some libraries:

```{r, echo=TRUE, include=TRUE, warning=FALSE, message=FALSE}
library(bnlearn)
library(tibble)
```

Then, we create the data just as in the exercise:
```{r}
df <- data.frame(
  A = as.character(c(0, 0, 0, 1, 1, 0, 1, 1, 1)),
  B = as.character(c(0, 0, 0, 1, 0, 0, 1, 0, 0)),
  C = as.character(c(0, 0, 0, 1, 1, 0, 1, 0, 0)),
  D = as.character(c(0, 0, 0, 1, 1, 1, 1, 0, 1)),
  E = as.character(c(0, 1, 1, 0, 0, 1, 0, 1, 1))
)

as.tibble(df)
```

We define the Bayesian network structure also as in the exercise (we could also learn this from the data):
```{r}
dag <- empty.graph(toupper(letters[1:5]))
arc.set <- matrix(
  c("A", "B", "A", "D", "B", "C", "E", "B"),
  ncol = 2, byrow = TRUE,
  dimnames = list(NULL, c("from", "to")))
arcs(dag) <- arc.set
plot(dag)
```

Then, we compute the local probability tables (the parameters):
```{r}
  fit <- bn.fit(dag, df)
  fit
```

With this, we are finally able submit some *queries* find the maximum conditional probability as in the exercise:
```{r}
  cpquery(fit, (A == "1"), (C == "1"))
  cpquery(fit, (A == "0"), (C == "1"))

  cpquery(fit, (E == "1"), (C == "1"))
  cpquery(fit, (E == "0"), (C == "1"))
```


<br><br><br>&copy; *S. D.* (<a href="https://github.com/dirmeier/probabilistic-models-introduction">GitHub</a>)
